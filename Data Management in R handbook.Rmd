---
title: "Data Management course handbook"
author: "Susan Jarvis"
date: "23 April 2016"
output: html_document
---

This is the accompanying material for the BES Quantitative Ecology Special Interest Group course on Data Management in R, delivered on 23rd May 2016 at Charles Darwin House, London.

The course is accompanied by a Powerpoint presentation (provided on Github in Rpres format) and covers three key areas:

1. Working with data from different sources in R

2. Managing data within R

3. Managing R scripts and datasets

The only pre-requisite for this course is that you have some experience with using R and know some basic commands. All of the commands used in this course will be demonstrated in this document so you can copy and paste commands, or write them in.

I recommend the use of RStudio as an interface to R as it provides a simple interface with Git and allows you to edit different types of scripts (R scripts, Markdown documents, Python scrips) in one place. 

You will need to have the following packages installed to run all the examples in this course:

* `sqldf` 

* `xlsx`

* `RODBC`

* `dplyr`

* `reshape2`

This course is written with a Windows environment in mind, if you are working with a Mac or Linux environment some commands, particularly in the first section, may not apply.


***IMPORTANT*** - don't copy and paste commands from the handbook directly into your R console. Always create a new R Script and paste/write all commands into this, then save. This means you can come back to your script at a later date and should be able to replicate your analysis exactly.


##1. Utilising data from different sources in R

For the first few exercises we will be using a made-up dataset which is presented in 3 different formats on [Github](https://github.com/susanjarvis501/Data-Management-in-R) (csv, Excel and Access). These are the most commonly used formats for holding data. To download each dataset, click on the link to the dataset and then click on 'View Raw' - this should download a copy of the dataset onto your machine. Note there are two .csv files to download.

If you are not working on Windows, you should be able to follow the .csv example but may not be able to work with the .xlsx and .accdb files.

###1.1 Excel

Excel is still the most commonly used program for data input and storage. Although Excel is often used for calculations or plotting, it is poorly suited to creating a reproducible analytical workflow and therefore it is recommended that Excel is only used as a means of entering data and storing raw data. 

Two main ways for importing data from Excel:

a) Export to a csv file and then import using read.csv

This is my preferred way of getting data from Excel into R. Comma separated files can be accessed by a wide range of programs and are therefore useful for transferring data between programs. They can also easily be transferred between operating systems (i.e. a csv file is recognised on both Windows and Linux). Tab-delimited files (.txt) can also be used but are less convenient if working between R and Excel as Excel does not immediately open them in spreadsheet form.

```{r, echo=FALSE}
setwd("N:/Quant Ecol SIG/")
```

```{r}
##I always find it helps to set the working directory near the top of your script
setwd("YOUR WORKING DIR")


#read.csv default settings assume your data has headers in the first row
datacsv1 <- read.csv("DM_2305_ExcelExample_plots.csv")
datacsv2 <- read.csv("DM_2305_ExcelExample_sites.csv")
#look at the top 6 rows of the first sheet
head(datacsv1)
```

b) Import directly from Excel, one sheet at a time

It is also possible to read in directly from the Excel file. This option is better if you are working with an Excel file that is constantly updated; you do not want to clog up your machine with multiple csv files.

```{r, message = FALSE}
#install and import the 'xlsx' package
require(xlsx)
#read the first sheet with plot measurements

dataexcel1 <- read.xlsx("DM_2305_ExcelExample.xlsx", sheetName = "Plot measurements")
#look at the top 6 rows of the first sheet
head(dataexcel1)
```

Exercise: 
1. Read in the second sheet from Excel

2. What is the difference between the output of the two methods?





###1.2 Access and Oracle databases

Access (and other database formats) are often used to store data from larger projects, where multiple people contribute towards the dataset. Unlike Excel, Access allows separate datasets to be linked to each other through joins and is therefore useful when a project collects multiple types of data.

When reading data into R we can interact directly with the database to pull out data without creating any intermediary copies. This is useful when the database is frequently updated, allowing you to access the newest dataset each time you pull the data into R. As databases are often very large, you also rarely want to import the entire database to R.

To interact with databases we need to use the `ROBDC` package.

```{r, message=FALSE}
library(RODBC)
```

Follow the instructions below to set up an ODBC connection on your computer. 

1. Find out the R version you are using with sessionInfo()

2. If you are using 64-bit R, switch to 32-bit (RStudio Tools > Global Options > General > R version > Change... and choose a 32-bit version) You will need to restart your R session

3. Search your C drive for 'odbcad32'and open the version stored in the SysWOW64 folder: C:\Windows\SysWoW64\odbcad32.exe	

4. Click Add > Microsoft Access Driver (*.mdb, *.accdb) > Finish. Input "Example" as the short name and description and then Select... > Navigate to where you downloaded the example database. Close the ODBC connector

5. Now we've set up the connection we can access it in R via the `RODBC` package.


Firstly, we need to create the connection using `odbcConnect` and the name of the database ("Example" in this case).

```{r, message=FALSE}
con <- odbcConnect("Example")
```

We can then read tables from the database into R.

```{r}
#List the tables in the database
sqlTables(con)
#Extract the plot measurement table
dataodbc1 <- sqlFetch(con, "Plot measurements")
head(dataodbc1)
```

Note that the Plot measurements table now has an extra column called 'ID' - this is because Access uses it's own unique ID system.

Alternatively, we might want to import specific records. This is often useful if we are dealing with very large databases with thousands of records and don't need to use all of the data.

To select specific records we use the SQL language which is commonly used in database applications.

```{r}
#Only select records from the first site
dataodbc1a <- sqlQuery(con, "select * from `Plot measurements` where Site = 1")
```

Basic SQL queries have three main components: select tells you which columns to select, in this case `*` means all columns, from states from which table and the where clause gives the condition to use (in this case where the column 'Site' is equal to 1). Note that you are directly querying the database without having to import the entire table into R, this is very helpful when the tables are large.

Exercise:

1. Select all columns from the Site climate table where temperature is higher than 15 degrees

2. Edit the query from exercise 1 to display only Site and Rainfall columns. Hint: the SQL equivalent of Table$Column is Table.Column and you can select multiple columns in a comma separated list e.g. Table.Column1, Table.Column2

3. How could the format of this database be improved?



###1.3 SAS

SAS is an alternative program used for databasing and general data analysis tasks which allows either scripting or a graphical interface.

It is possible to directly read SAS database files using either the `haven` or `sas7bdat` package HOWEVER as both methods require export of data as a SAS dataset it is probably just as efficient to export as .csv and import using `read.csv`.



###1.4 NetCDF

NetCDF is a common data format for gridded time series datasets, such as climate or nitrogen deposition data. 

Packages`RNetCDF` and `ncdf4` can read NetCDF files into R.

[Useful guide to netCDF with R](http://disc.sci.gsfc.nasa.gov/recipes/?q=recipes/How-to-Read-Data-in-netCDF-Format-with-R)



###1.5 Spatial data

Lots of R packages available such as `rdgal`, `rgeos` and lots of tutorials available such as this one:

https://github.com/Robinlovelace/Creating-maps-in-R

***To be updated with info from the QE SIG Spatial Data Course***



##2. Manipulating data in R

###2.1 Single datasets

For all these examples we will use the iris dataset distributed as part of base R, this is a dataset of measurements of flower characteristics for three species of iris. 

Manipulations of single datasets are generally conducted with the aim of 'tidying up' data for analysis. Hadley Wickham has written a paper on this called [Tidy Data](http://vita.had.co.nz/papers/tidy-data.pdf) linked to the `tidyr` package with some excellent guidelines to point you through this process. We won't use the `tidyr` or `dplyr` packages much in this course, as most of the functions do the same job as more basic functions which we will cover, but you might want to investigate these as well.


####2.1.1 Formats

There are lots of situations in R where the format of your data is crucial i.e. some functions will only work with dataframes, some will only work with matrices. The data import methods in part 1 almost always produce a dataframe but it is good practice to check the format after importing any data to make sure. If you are using RStudio you can check the format of any data in the Environment window. You can also check the format using `is.` functions.

Import iris data so it shows up in your environment window if you are using RStudio.
```{r}
iris <- iris
```
The little table symbol tells us the format is dataframe but we can check directly
```{r}
is.data.frame(iris)
is.matrix(iris)
```
The `typeof` function also gives the format...
```{r}
typeof(iris)
```
But beware! Dataframes are represented as lists in R so you might not get the answer you expect! `is.` statements are the better option here.
If we needed the iris dataset as a matrix or list we can use `as.`
```{r}
iris.mat <- as.matrix(iris)
iris.list <- as.list(iris)
```

Most of the time you will be using data in a dataframe. Dataframes can hold columns of values of different types e.g. numeric, character and factor variables. Matrices cannot do this (note all the values in iris.mat are of character type). Many common R errors arise from not checking the data type of columns in a dataframe.

For example, lets add a new factor (experimental plot let's say, to which plants were randomly assigned) to the iris dataset
```{r}
iris$Plot <- rep(c(rep(1,10), rep(2, 10), rep(3,5)),3)
```
We want to run a linear model with Species and Plot as predictors of Sepal.Length (fixed effects)
```{r}
lm1 <- lm(Sepal.Length ~ Species + Plot, data = iris)
summary(lm1)
```
The model has correctly intepreted Species as a factor (categorical) variable but has assumed plot is numeric and calculated a slope. This doesn't make sense as plot is not a measured covariate (plot 3 is not 3x "plottier" than plot 1!).

So we need to make sure that plot is also recognised as a factor. `is.` statements can help us here too.
```{r}
is.factor(iris$Plot)
iris$Plotf <- factor(iris$Plot)
is.factor(iris$Plotf)

lm2 <- lm(Sepal.Length ~ Species + Plotf, data = iris)
summary(lm2)
```


Lists are a very handy format for storing data of all sorts of types (vectors, dataframes, matrices etc).

List structure can be complicated with several levels. Return list structure with `str`.
```{r, eval = FALSE}
str(iris.list)
```
Retrieve list elements with `[]` or `[[]]` if unnamed, or `$` if the list elements are named.
```{r, eval = FALSE}
iris.list$Sepal.Length
iris.list[1]
iris.list[[1]]
iris.list[[1]][1]
```
Note that lists often have multiple levels and combinations of square brackets can be used to extract elements from different levels.

Tip: if you are creating a lot of, for example, dataframes as part of your workflow, it can be useful to store these as elements of a list. That way you can reference them as list elements instead of assigning them all unique names.


####2.1.2 Sorting data

Sorting data in R, assuming your data are in a dataframe, uses the `order` function. Let's say we want to sort the iris data by Petal.Width, with narrow petals first.

```{r, eval=FALSE}
iris[order(iris$Petal.Width),]
```
Or with wide petals first
```{r, eval=FALSE}
iris[order(-iris$Petal.Width),]
```
Or by Species, then Petal.Width
```{r, eval=FALSE}
iris[order(iris$Species, iris$Petal.Width),]
```


####2.1.3 Removing duplicates

Duplicate entries can be a big problem in analysis, particularly if you are using big datasets where duplicates are difficult to spot by eye. Duplicates can arise for lots of reasons: bad data entry, accidental duplication during data manipulation in Excel (dragging cells), wrongly matching up data in SAS etc. Note: if you have duplicate entries always go back and find out why! Often they are an indication of a bigger problem in your data workflow.

Checking for duplicates uses `duplicated`. By default this looks for duplicates across all columns of a dataframe (i.e. all entries in a row match another row exactly).

```{r, eval=FALSE}
duplicated(iris)
```
Duplicated returns a TRUE/FALSE value for every row indicating whether it is identical to another row, all the rows in iris data are unique. However, what if we were expecting a dataset where combinations of Petal.Length and Petal.Width were all unique.
```{r, eval=FALSE}
duplicated(iris[,3:4])
```
Note the column numbers were used to identify the relevant columns for concise coding. We now have some duplicates. We can investigate this further by looking at the first few rows to see what the function is doing
```{r}
duplicated(iris[,3:4])[1:6] #first six results of duplicated
head(iris,6)#first six rows of the dataset
```
The function tells us that the second and fifth rows are duplicated. Loking at the dataset we can see that both of these rows have Petal.Length of 1.4 and Petal.Width of 0.2, the same as the row 1. However, row 1 does not count as duplicated - the function only identifies second entries onwards. This means we can easily use the function to remove the duplicate entries while retaining a single entry for this combination from row 1:

```{r, eval=FALSE}
iris.unique <- iris[!duplicated(iris[,3:4]),] #The exclamation marks means 'not'
nrow(iris.unique) #102 rows remain in this dataset from the 150 original rows
```

The same result can also be achieved using the `distinct` function in package `dplyr`:

```{r, warning = FALSE, message = FALSE}
require(dplyr)
iris.unique2 <- distinct(iris, Petal.Length, Petal.Width)
nrow(iris.unique2)
```


####2.1.4 Removing missing data

Although most R functions can cope with `NA` values using either `na.action` or `na.rm` arguments, there are some cases where it might be necessary to remove rows that contain `NA`. There is a simple function `complete.cases` to do this.

The iris data currenly doesn't have any `NA` values so let's create a new dataset and replace some values with NA, such as might occur if measurements were missing.
```{r}
iris.NA <- iris
iris.NA[1:4,1] <- NA #replace the first four entries in column 1 with NA
head(iris.NA)
```
We can identify quickly which columns contain `NA` using `summary`.
```{r, eval=F}
summary(iris.NA)
```
And display rows containing `NA` values in the Sepal.Length column
```{r}
iris.NA[is.na(iris.NA$Sepal.Length),]
```
If we decide we need to remove rows containing `NA` we can select complete cases only
```{r, eval=F}
iris.NA.cc <- iris.NA[complete.cases(iris.NA),]
head(iris.NA.cc)
summary(iris.NA.cc)
```
Rows 1:4 have now been removed and there are no `NA` values in the table.


####2.1.5 Reshaping data

There may be times when you are presented with data that is not in an ideal format for analysis e.g. multiple variables in a single column. Hadley Wickham defines 'tidy data' as having the following characteristics (check out his paper in the Journal of Statistical Software (2014) 59 for more details):

1. Each variable is a column

2. Each observation is a row

3. Each type of observational unit forms a table

There is a whole R package `tidyr` with a range of functions to help tidy up datasets. There is a really brief and useful introduction to the key functions in the package here: https://blog.rstudio.org/2014/07/22/introducing-tidyr/

However, I prefer to use the older `reshape` and `reshape2` packages which are not as simple but more flexible. Here we will use the newer `reshape2` package.

```{r, warning=FALSE,message=FALSE}
require(reshape2)
```

There are only two key functions in this package: `melt` and `cast`.

`melt` is used to change dataframes from wide to long format. For example, applying this to the iris dataset gives us a single column with all values in and another column with all the variable labels. By default melt uses factor variables as 'id' variables (look at the difference between how Plotf and Plot columns are dealt with). 

```{r, message=FALSE}
iris.melt <- melt(iris)
summary(iris.melt)
```

`cast` is used to change long format to wide format (where each variable as a column). In `reshape2` there are two options: `acast` to create matrices and `dcast` to create dataframes.

You can now choose a range of functions with which to summarise data, the default option is length which calculates a frequency table of the values.

```{r, message=FALSE}
iris.cast <- dcast(iris.melt, value~variable)
head(iris.cast)
```

This might not be what we are looking for, it might be more sensible to use the cast function to calculate a mean of each of the variables, per species and plot combination.

```{r, message=FALSE}
iris.cast2 <- dcast(iris.melt, Species~variable, fun=mean)
iris.cast2
```

Note that in both cases of using `cast` functions the structure is denoted by a formula, with rows on the left hand side and columns on the right. 


####2.1.6 Summarising data

We saw above how we could summarise data using `cast`. Two other popular approaches are `aggregate` and `apply`.

Using the same example of calculating means for each variable for each species in the iris dataset:

```{r,warning = FALSE,message =FALSE}
iris.agg <- aggregate(iris,list(iris$Species),mean)
```

Note the default output gives you warnings about not removing factor variables first, try removing these using `[]` to subset columns and iris.agg should be the same as iris.cast2.

`tapply` can be used to calculate group means but only works on a single column at a time.

```{r, eval=FALSE}
tapply(iris$Sepal.Length,iris$Species,mean)
```


####2.1.7 Applying functions to data

I often hear people tell me that they have Excel spreadsheets set up to automatically run functions to, for example, convert units from lab or field outputs. While automatic calculation is preferable to manual, doing this is still less preferable than a direct import to R where you can do these calculations in an easily reproducible and auditable way.

R is (very handily) vectorized which means it is very simple to calculate functions for each row of a dataset/entry in a vector.

For example we can create a new column with log(Sepal.Length):
```{r}
iris$LogSepLength <- log(iris$Sepal.Length)
```

For more complicated functions you might occasionally need to use `apply` functions, `if` statements or loops. There is a lot of information available on these so we won't cover them in detail here.


Exercise:

1. So far we have tried these dataset manipulations on an already "perfect" dataset. Try removing any rows with missing data and then formatting using the tidy data guidelines the data imported from the Plot measurements table. What are the issues that arise? Edit your script to solve the problem(s).

2. Install the `tidyr` package and use this with the `dplyr` package using the   [cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) to replicate the single dataset manipulations with these packages. 



###2.2 Multiple datasets

Often you will not store all your data in one dataframe. For example, you might have collected data on two separate occasions, each sampling time being stored in a different dataframe. Or you might keep information about say, climate conditions, in a separate sheet from data about soil chemistry. We will now investigate ways of joining data, still using the iris dataset.

####2.2.1 Appending data

Firstly, we will consider a scenario where we had additional measurements from a fourth plot of iris data, which was not included in the original dataset. Only one species was measured in this plot.
```{r}
#create some new random data for plot 4
iris.extra <- data.frame(Sepal.Length = rnorm(10, 5, 0.7), Sepal.Width = rnorm(10,3.2,0.5),Petal.Length = rnorm(10,1.3,0.3), Petal.Width = rnorm(10,0.2, 0.001), Species = "setosa", Plot = 4)
```

The function `rbind` is the basic function in R to append datasets.
```{r, eval=F}
rbind(iris, iris.extra)
```

However, `rbind` requires the correct number of columns - we have two extra columns in the iris data. We therefore need to either add these new columns to the new data or remove them from the existing data. Let's try adding them to the new data
```{r}
iris.extra$Plotf <- factor(iris.extra$Plot)
iris.extra$LogSepLength <- log(iris.extra$Sepal.Length)
iris.all <- rbind(iris, iris.extra)
```
We are now able to combine the datasets with `rbind`. Alternative tools are `smartbind` in the `gtools` package or `rbind.fill` from `plyr` which will add `NA` values if columns cannot be matched.



####2.2.2 Matching data

The alternative situation you will commonly encounter is when different data types are stored in different dataframes but can be linked through common observations. For example, observations of individual weight in one dataframe with other characteristics about that individual (years of education) in a different dataframe. This situation is often useful when one characteristic (such as weight) is measured multiple times without influencing the other variable. If all data were in one dataframe then the values for e.g. years of education would have to be duplicated for each new measurement of mass.

Keeping with the iris dataset, let us imagine that we also have two extra datasets with different types of data. The first has information about the species (average height, colour), the second has information about the seeds collected from each plant (number of seeds, proportion germinated).

You will note that there is now a big problem with the iris dataset - no unique and persistent identifier! Note that this hasn't been a problem until now, but we now have no observation level identifier to join to a new dataset, so we will have to make one. It might make sense to include the species name in the identifier so we don't have to keep referencing back to remember which number is which species so we'll number individuals "setosa1" etc. We could have chosen a different ID scheme e.g. including plot as well, the key thing is that this is unique for each observation. We will assume individual plants were only measured once.

```{r}
iris.all$ObsID <- paste0(iris.all$Species, row.names(iris.all))
```

Now we can create our two additional datasets. Firstly the species data.
```{r}
irisspdata <- data.frame(Species = unique(iris$Species), avgheight = c(42.3, 33.5, 35.7), colour = c("violet", "blue", "blue"))
```

And then the additional plant data (generated randomly for this example).
```{r}
irisindivdata <- data.frame(ObsID = iris.all$ObsID, noseeds = c(rpois(50, 10), rpois(50,8), rpois(50, 9), rpois(10,10)))
irisindivdata$germprop <- c(rpois(50, 3), rpois(50,2), rpois(50, 3), rpois(10,3))/irisindivdata$noseeds
```

There are many ways to match datasets in R but I find the most flexible is to use the package `sqldf` which allows SQL based queries in R. This is very useful if you want to replicate an SQL query from another program such as SAS in R as the syntax will be the same. 

```{r, message = FALSE, warning = FALSE}
require(sqldf)
```
Joining the species data is straightforward BUT `.` means something different in SQL so I first need to rename iris.all as iris_all

```{r}
iris_all <- iris.all
irismatchsp <- sqldf("select * from iris_all, irisspdata where iris_all.Species = irisspdata.Species")
```

The `*` indicate we want all columns from both datasets in the new dataset.

Now we can add the other species data.

```{r,eval=FALSE}
irismatchindiv <- sqldf("select * from irismatchsp, irisindivdata where irismatchsp.ObsID = irisindivdata.ObsID")
```

Or not...now we have issues because we have duplicate columns in our data which actually orignate from the first join. Lets try that again but using only the required columns

```{r}
irismatchsp2 <- sqldf("select t1.*, t2.avgheight, t2.colour from iris_all as t1, irisspdata as t2 where t1.Species = t2.Species")
irismatchindiv <- sqldf("select * from irismatchsp2, irisindivdata where irismatchsp2.ObsID = irisindivdata.ObsID")
head(irismatchindiv)
```

We have now successfully joined all our data together. Note the two step process - this is useful to do to make sure you catch any errors as you go.


Exercise:

1. Join the Plot measurements and Site climate tables from Part 1 to select measurements of Vars 1 and 2 which came from high rainfall sites. Hint: visualise the distribution of rainfall data to select your high rainfall sites.

2. Try to combine the two steps in the iris data joining example into one single `sqldf` query.


## 3. Documentation

Script documentation is becoming more and more important to enable analyses to be shared, reproducible and auditable. Journals already ask you to document data and many are also moving to ask you to document scripts so its a good habit to get into.

This script is documentated in the following ways:

1. Code is commented where neccessary with `#`

2. The code is embedded in a [Markdown document](http://rmarkdown.rstudio.com/)

3. The code is stored on [Github](https://github.com/)


Install links for Git:

Windows: http://git-scm.com/download/win
OS X: http://git-scm.com/download/mac
Debian/Ubuntu: sudo apt-get install git-core
Other linux: http://git-scm.com/download/linux



##4. Useful links

Below are a list of projects and resources you might find useful in the broad area of R and Data Management. Most of these are tools I use in my everyday work as a data analyst/statistician/ecologist (RODBC, SQL, Git, Markdown, Shiny apps) and there's a couple of things like OpenRefine I've not investigated yet but look interesting. There is a whole world of resources out there, don't feel like you need to know everything (I don't!) but use what you need and what you find works for you.

[Data carpentry lesson on spreadsheets](https://github.com/datacarpentry/spreadsheet-ecology-lesson/blob/gh-pages/ecology_spreadsheets.md)

[OpenRefine is a useful tool for messy data](http://openrefine.org/)

[RODBC package](https://cran.r-project.org/web/packages/RODBC/RODBC.pdf)

[Intro to SQL](https://www.khanacademy.org/computing/computer-programming/sql)

[BES QE SIG intro to Git](http://bes-qsig.github.io/fge/docs/git_basics/)

[How to build an R package](http://bes-qsig.github.io/fge/docs/how_to_build_an_r_package/)

[Introduction to Markdown](http://bes-qsig.github.io/fge/docs/introduction_to_markdown/)

[How to use Markdown in R (RMarkdown)](http://rmarkdown.rstudio.com/)

[How to build an app in R with Shiny](http://shiny.rstudio.com/)

[Pipelines for data analysis in R](https://www.rstudio.com/resources/webinars/pipelines-for-data-analysis-in-r/)
